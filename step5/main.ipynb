{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1c9d8f",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1762103751376,
     "user": {
      "displayName": "김현수",
      "userId": "02831780464337922222"
     },
     "user_tz": -540
    },
    "id": "ca1c9d8f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    modeling_outputs,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, log_loss\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70836f00",
   "metadata": {},
   "source": [
    "# Step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e812c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA\n",
      "Using model: microsoft/deberta-v3-small\n",
      "학습 데이터: 57477, 테스트 데이터: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/mlp/venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5378ee780314f5f943a92eb603149a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c150e8229f2453d98c6d5e7811465f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 8e-5\n",
    "EPOCHS = 3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device.upper()}\")\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(\"./data/train.csv\")\n",
    "    test_df = pd.read_csv(\"./data/test.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"파일을 찾을 수 없습니다: {e}\")\n",
    "\n",
    "def create_target(row):\n",
    "    if row['winner_model_a'] == 1:\n",
    "        return 0  # Class 0: A wins\n",
    "    if row['winner_model_b'] == 1:\n",
    "        return 1  # Class 1: B wins\n",
    "    if row['winner_tie'] == 1:\n",
    "        return 2  # Class 2: Tie\n",
    "    return -1\n",
    "\n",
    "train_df['label'] = train_df.apply(create_target, axis=1)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"학습 데이터: {len(train_dataset)}, 테스트 데이터: {len(test_dataset)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    sep = tokenizer.sep_token\n",
    "    texts_a = [\n",
    "        f\"prompt: {p} {sep} response A: {a}\"\n",
    "        for p, a in zip(examples['prompt'], examples['response_a'])\n",
    "    ]\n",
    "    texts_b = [\n",
    "        f\"prompt: {p} {sep} response B: {b}\"\n",
    "        for p, b in zip(examples['prompt'], examples['response_b'])\n",
    "    ]\n",
    "\n",
    "    tokenized_a = tokenizer(\n",
    "        texts_a,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    tokenized_b = tokenizer(\n",
    "        texts_b,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    tokenized_inputs = {\n",
    "        'input_ids_a': tokenized_a['input_ids'],\n",
    "        'attention_mask_a': tokenized_a['attention_mask'],\n",
    "        'input_ids_b': tokenized_b['input_ids'],\n",
    "        'attention_mask_b': tokenized_b['attention_mask'],\n",
    "    }\n",
    "    if 'label' in examples:\n",
    "        tokenized_inputs[\"labels\"] = examples[\"label\"]\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aUtXzTHbj4Rt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 772,
     "referenced_widgets": [
      "273befe26bd44e2d9238f5d9b346b3c2",
      "4946c97d5cb84987bec9a59706ded064",
      "1a6378510a8546758b84e12f4ea96436",
      "8ba98f5e6bd344b3aa043057ce07d01f",
      "5d6f878eaaca4b17bc38e5c30ac429ea",
      "fa8658d501bc4ed4b48adee306a7e356",
      "1563cea5ba41473c90113e41a35f7ea9",
      "56cf6b6f6e7d40ab9a2142f1679f5841",
      "d6c37e810dd745a1b4ea09318b20bba7",
      "dee928c8e02c4d7883040513ffb7a44d",
      "00dc32be2b554547afa3795e6bdf34a2",
      "0fa16773008644ae806b1b75bcf8754b",
      "cc22b21cd1564d7a90d66e9c9506f43a",
      "b563e5e91230450aa521b522272ebea5",
      "66fe7fbd1d214373bf8f8bd98057bfb1",
      "423ad24f44f04a1499b517d6ee36ec10",
      "803aae63d33447988d7242ed9563039f",
      "7b4f3405e5974d44b2c7f56fa09aea5e",
      "d574f662152e47059ef34c31f209a5b2",
      "ed6960043f264b30a73ca7ddcc56a1e6",
      "f0b8d5408a734e3a9e81e37db461c4a4",
      "b2a7d84d6ae1423ab35211a8b94d325b",
      "0a9524be087b48e788e3b424c89a71f9",
      "be0a7cd8f9b64d70885844bf055edbd2",
      "d9d3d5fa1fcd4cce81d8c6aa1a10672e",
      "baf8f006d922435b89a8de7ae949d305",
      "9c95cb0f41c1476394cd0627ff4bc469",
      "7d28d92dac8a4efb862baf9925d9cb1d",
      "e5521bdd6918494e9970bf5f92131edb",
      "3c50a8e4c87d4c43bff29adffd76bd22",
      "eeb673b9e2b444e5afe79568168008af",
      "e72fa8d2c9dc42478b8ef263187906fd",
      "12b16a77e8d74294a1d4bc4e2033291e",
      "a7ec3b77f37441ca9ea4a36a5495a7c0",
      "54f2884dda2246aaa51173aa4d307291",
      "befd758b94944584b7a4f45dc0814dfc",
      "d69cb1a8cb9c489b8d31a3005080ab09",
      "281ea5804e424721baf5767534cf44c3",
      "b9aa0a3204734743849c0ed89cb89211",
      "6f2a69af899146da9233efe7261a32cb",
      "6ca22a397b7c4cefb90fd11c9d285630",
      "d125b32253b245c7914a996710a087b4",
      "a381346233bd4e13b83e8d6193a7f447",
      "4a3f394ddf6f403e98d6562cf49e7eb0"
     ]
    },
    "executionInfo": {
     "elapsed": 423923,
     "status": "ok",
     "timestamp": 1762106671170,
     "user": {
      "displayName": "김현수",
      "userId": "02831780464337922222"
     },
     "user_tz": -540
    },
    "id": "aUtXzTHbj4Rt",
    "outputId": "256fc03b-923d-4d55-bc09-318d547816eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 141,599,232 || trainable%: 0.2083\n",
      "Trainer 설정 중...\n",
      "--- 학습 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5391' max='5391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5391/5391 09:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.006100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 학습 완료 ---\n",
      "테스트 데이터 예측 중...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 예측 확률 Shape: (3, 3)\n",
      "제출 파일 'submission_step5_deberta_lora.csv' 생성이 완료되었습니다.\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.265886        0.274929    0.459185\n",
      "1   211333        0.299775        0.369991    0.330234\n",
      "2  1233961        0.225737        0.489218    0.285045\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_proj\", \"value_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_base_model = get_peft_model(base_model, lora_config)\n",
    "peft_base_model.print_trainable_parameters()\n",
    "\n",
    "class DeBERTaClassifier(torch.nn.Module):\n",
    "    def __init__(self, peft_model, num_labels=3):\n",
    "        super().__init__()\n",
    "        self.peft_model = peft_model\n",
    "        hidden_size = self.peft_model.config.hidden_size \n",
    "        self.num_labels = num_labels\n",
    "        self.cross_attn = torch.nn.ModuleList([\n",
    "            torch.nn.MultiheadAttention(\n",
    "                embed_dim=hidden_size,\n",
    "                num_heads=8,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ) for _ in range(6)\n",
    "        ])\n",
    "        \n",
    "        self.classifier_head = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(hidden_size * 2, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(hidden_size, self.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids_a=None, attention_mask_a=None, \n",
    "                input_ids_b=None, attention_mask_b=None, \n",
    "                labels=None):\n",
    "        \n",
    "        tokens_a = self.peft_model(\n",
    "            input_ids=input_ids_a,\n",
    "            attention_mask=attention_mask_a\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        tokens_b = self.peft_model(\n",
    "            input_ids=input_ids_b,\n",
    "            attention_mask=attention_mask_b\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        for cross_attn_layer in self.cross_attn:\n",
    "            attn_output_a, _ = cross_attn_layer(\n",
    "                query=tokens_a,\n",
    "                key=tokens_b,\n",
    "                value=tokens_b,\n",
    "                key_padding_mask=(attention_mask_b == 0)\n",
    "            )\n",
    "\n",
    "            attn_output_b, _ = cross_attn_layer(\n",
    "                query=tokens_b,\n",
    "                key=tokens_a,\n",
    "                value=tokens_a,\n",
    "                key_padding_mask=(attention_mask_a == 0)\n",
    "            )\n",
    "            tokens_a = attn_output_a\n",
    "            tokens_b = attn_output_b\n",
    "        \n",
    "        pooled_output_a = tokens_a[:, 0]\n",
    "        pooled_output_b = tokens_b[:, 0]\n",
    "        \n",
    "        combined_output = torch.cat((pooled_output_a, pooled_output_b), dim=1)\n",
    "        \n",
    "        logits = self.classifier_head(combined_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) \n",
    "        \n",
    "        return modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "\n",
    "model = DeBERTaClassifier(peft_base_model, num_labels=3).to(device)\n",
    "\n",
    "class DualEncoderDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \n",
    "        features_a = []\n",
    "        features_b = []\n",
    "        labels = []\n",
    "\n",
    "        for feature in features:\n",
    "            features_a.append({\n",
    "                'input_ids': feature['input_ids_a'],\n",
    "                'attention_mask': feature['attention_mask_a']\n",
    "            })\n",
    "            features_b.append({\n",
    "                'input_ids': feature['input_ids_b'],\n",
    "                'attention_mask': feature['attention_mask_b']\n",
    "            })\n",
    "            if 'labels' in feature:\n",
    "                labels.append(feature['labels'])\n",
    "\n",
    "        batch_a = self.tokenizer.pad(\n",
    "            features_a,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        batch_b = self.tokenizer.pad(\n",
    "            features_b,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {\n",
    "            'input_ids_a': batch_a['input_ids'],\n",
    "            'attention_mask_a': batch_a['attention_mask'],\n",
    "            'input_ids_b': batch_b['input_ids'],\n",
    "            'attention_mask_b': batch_b['attention_mask'],\n",
    "        }\n",
    "\n",
    "        if labels:\n",
    "            batch['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "dual_collator = DualEncoderDataCollator(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = F.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    epsilon = 1e-15\n",
    "    probs = np.clip(probs, epsilon, 1 - epsilon)\n",
    "    logloss = log_loss(labels, probs)\n",
    "    return {\"loss\": logloss}\n",
    "\n",
    "\n",
    "print(\"Trainer 설정 중...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final_2mlp\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    fp16=True if device == 'cuda' else False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=dual_collator\n",
    "\n",
    ")\n",
    "\n",
    "print(\"--- 학습 시작 ---\")\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "print(\"--- 학습 완료 ---\")\n",
    "\n",
    "\n",
    "# --- 6. 테스트 데이터 예측 및 제출 ---\n",
    "print(\"테스트 데이터 예측 중...\")\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "\n",
    "# Logits -> Probabilities\n",
    "test_logits = predictions.predictions\n",
    "test_probs = F.softmax(torch.tensor(test_logits), dim=-1).numpy()\n",
    "\n",
    "print(f\"테스트 예측 확률 Shape: {test_probs.shape}\")\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission_df = pd.DataFrame({'id': test_df['id']})\n",
    "submission_df['winner_model_a'] = test_probs[:, 0] # Class 0\n",
    "submission_df['winner_model_b'] = test_probs[:, 1] # Class 1\n",
    "submission_df['winner_tie']     = test_probs[:, 2] # Class 2\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"제출 파일 'submission_step5_deberta_lora.csv' 생성이 완료되었습니다.\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
