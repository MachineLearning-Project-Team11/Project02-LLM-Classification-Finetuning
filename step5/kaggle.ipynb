{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":13613448,"sourceType":"datasetVersion","datasetId":8651468},{"sourceId":631835,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":476181,"modelId":492101}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:40:19.352435Z","iopub.execute_input":"2025-11-06T11:40:19.353137Z","iopub.status.idle":"2025-11-06T11:40:19.377746Z","shell.execute_reply.started":"2025-11-06T11:40:19.353104Z","shell.execute_reply":"2025-11-06T11:40:19.376885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    modeling_outputs,\n    TrainingArguments,\n    Trainer\n)\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, log_loss\nfrom peft import get_peft_model, LoraConfig\nfrom torch.nn import functional as F\nfrom safetensors.torch import load_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:38:36.406212Z","iopub.execute_input":"2025-11-06T11:38:36.406568Z","iopub.status.idle":"2025-11-06T11:39:06.449515Z","shell.execute_reply.started":"2025-11-06T11:38:36.406548Z","shell.execute_reply":"2025-11-06T11:39:06.448756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"/kaggle/input/deberta-ft/pytorch/default/2\"\nMAX_LENGTH = 512\nBATCH_SIZE = 8\nLEARNING_RATE = 2e-5\nEPOCHS = 3\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device.upper()}\")\nprint(f\"Using model: {MODEL_NAME}\")\n\ntry:\n    train_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\nexcept FileNotFoundError as e:\n    print(f\"파일을 찾을 수 없습니다: {e}\")\n\ndef create_target(row):\n    if row['winner_model_a'] == 1:\n        return 0  # Class 0: A wins\n    if row['winner_model_b'] == 1:\n        return 1  # Class 1: B wins\n    if row['winner_tie'] == 1:\n        return 2  # Class 2: Tie\n    return -1\n\ntrain_df['label'] = train_df.apply(create_target, axis=1)\n\n# train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label'])\n\ntrain_dataset = Dataset.from_pandas(train_df)\n# val_dataset = Dataset.from_pandas(val_data)\ntest_dataset = Dataset.from_pandas(test_df)\n\nprint(f\"학습 데이터: {len(train_dataset)}, 테스트 데이터: {len(test_dataset)}\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/deberta-v3-small/deberta-v3-small\")\n\ndef preprocess_function(examples):\n    sep = tokenizer.sep_token\n    texts_a = [\n        f\"prompt: {p} {sep} response A: {a}\"\n        for p, a in zip(examples['prompt'], examples['response_a'])\n    ]\n    texts_b = [\n        f\"prompt: {p} {sep} response B: {b}\"\n        for p, b in zip(examples['prompt'], examples['response_b'])\n    ]\n\n    tokenized_a = tokenizer(\n        texts_a,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\"\n    )\n    \n    tokenized_b = tokenizer(\n        texts_b,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\"\n    )\n\n    tokenized_inputs = {\n        'input_ids_a': tokenized_a['input_ids'],\n        'attention_mask_a': tokenized_a['attention_mask'],\n        'input_ids_b': tokenized_b['input_ids'],\n        'attention_mask_b': tokenized_b['attention_mask'],\n    }\n    if 'label' in examples:\n        tokenized_inputs[\"labels\"] = examples[\"label\"]\n\n    return tokenized_inputs\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n# tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\ntokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:40:24.504504Z","iopub.execute_input":"2025-11-06T11:40:24.505261Z","iopub.status.idle":"2025-11-06T11:41:57.852325Z","shell.execute_reply.started":"2025-11-06T11:40:24.505236Z","shell.execute_reply":"2025-11-06T11:41:57.851517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = AutoModel.from_pretrained(\n    \"/kaggle/input/deberta-v3-small/deberta-v3-small\",\n    device_map=device\n)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"query_proj\", \"value_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n)\n\npeft_base_model = get_peft_model(base_model, lora_config)\npeft_base_model.print_trainable_parameters()\n\nclass DeBERTaClassifier(torch.nn.Module):\n    def __init__(self, peft_model, num_labels=3):\n        super().__init__()\n        self.peft_model = peft_model\n        hidden_size = self.peft_model.config.hidden_size \n        self.num_labels = num_labels\n        self.cross_attn = torch.nn.ModuleList([\n            torch.nn.MultiheadAttention(\n                embed_dim=hidden_size,\n                num_heads=8,\n                dropout=0.1,\n                batch_first=True\n            ) for _ in range(6)\n        ])\n        \n        self.classifier_head = torch.nn.Sequential(\n            torch.nn.Dropout(0.1),\n            torch.nn.Linear(hidden_size * 2, hidden_size),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.1),\n            torch.nn.Linear(hidden_size, self.num_labels)\n        )\n\n    def forward(self, \n                input_ids_a=None, attention_mask_a=None, \n                input_ids_b=None, attention_mask_b=None, \n                labels=None):\n        \n        tokens_a = self.peft_model(\n            input_ids=input_ids_a,\n            attention_mask=attention_mask_a\n        ).last_hidden_state\n        \n        tokens_b = self.peft_model(\n            input_ids=input_ids_b,\n            attention_mask=attention_mask_b\n        ).last_hidden_state\n        \n        for cross_attn_layer in self.cross_attn:\n            attn_output_a, _ = cross_attn_layer(\n                query=tokens_a,\n                key=tokens_b,\n                value=tokens_b,\n                key_padding_mask=(attention_mask_b == 0)\n            )\n\n            attn_output_b, _ = cross_attn_layer(\n                query=tokens_b,\n                key=tokens_a,\n                value=tokens_a,\n                key_padding_mask=(attention_mask_a == 0)\n            )\n            tokens_a = attn_output_a\n            tokens_b = attn_output_b\n        \n        pooled_output_a = tokens_a[:, 0]\n        pooled_output_b = tokens_b[:, 0]\n        \n        combined_output = torch.cat((pooled_output_a, pooled_output_b), dim=1)\n        \n        logits = self.classifier_head(combined_output)\n        \n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) \n        \n        return modeling_outputs.SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=None,\n            attentions=None,\n        )\n\nmodel = DeBERTaClassifier(peft_base_model, num_labels=3).to(device)\n\nload_model(model, \"/kaggle/input/deberta-ft/pytorch/default/2/model.safetensors\")\nclass DualEncoderDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, features):\n        \n        features_a = []\n        features_b = []\n        labels = []\n\n        for feature in features:\n            features_a.append({\n                'input_ids': feature['input_ids_a'],\n                'attention_mask': feature['attention_mask_a']\n            })\n            features_b.append({\n                'input_ids': feature['input_ids_b'],\n                'attention_mask': feature['attention_mask_b']\n            })\n            if 'labels' in feature:\n                labels.append(feature['labels'])\n\n        batch_a = self.tokenizer.pad(\n            features_a,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        \n        batch_b = self.tokenizer.pad(\n            features_b,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        batch = {\n            'input_ids_a': batch_a['input_ids'],\n            'attention_mask_a': batch_a['attention_mask'],\n            'input_ids_b': batch_b['input_ids'],\n            'attention_mask_b': batch_b['attention_mask'],\n        }\n\n        if labels:\n            batch['labels'] = torch.tensor(labels, dtype=torch.long)\n            \n        return batch\n    \ndual_collator = DualEncoderDataCollator(tokenizer=tokenizer)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = F.softmax(torch.tensor(logits), dim=-1).numpy()\n    epsilon = 1e-15\n    probs = np.clip(probs, epsilon, 1 - epsilon)\n    logloss = log_loss(labels, probs)\n    return {\"loss\": logloss}\n\n\nprint(\"Trainer 설정 중...\")\ninfer_args = TrainingArguments(\n    output_dir=\"./infer_results\",\n    per_device_eval_batch_size=BATCH_SIZE,\n    report_to=\"none\",\n    fp16=(device == 'cuda') # GPU 사용 시 fp16 활성화\n)\n\ntrainer = Trainer(\n    model=model,\n    args=infer_args,\n    data_collator=dual_collator\n    # tokenizer=tokenizer\n)\n\n# print(\"--- 학습 시작 ---\")\n# trainer.train()\n# print(\"--- 학습 완료 ---\")\n\n# --- 6. 테스트 데이터 예측 및 제출 ---\nprint(\"테스트 데이터 예측 중...\")\npredictions = trainer.predict(tokenized_test)\n\n# Logits -> Probabilities\ntest_logits = predictions.predictions\ntest_probs = F.softmax(torch.tensor(test_logits), dim=-1).numpy()\n\nprint(f\"테스트 예측 확률 Shape: {test_probs.shape}\")\n\n# 제출 파일 생성\nsubmission_df = pd.DataFrame({'id': test_df['id']})\nsubmission_df['winner_model_a'] = test_probs[:, 0] # Class 0\nsubmission_df['winner_model_b'] = test_probs[:, 1] # Class 1\nsubmission_df['winner_tie']     = test_probs[:, 2] # Class 2\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"제출 파일 'submission_step5_deberta_lora.csv' 생성이 완료되었습니다.\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:42:18.300047Z","iopub.execute_input":"2025-11-06T11:42:18.300823Z","iopub.status.idle":"2025-11-06T11:42:25.026388Z","shell.execute_reply.started":"2025-11-06T11:42:18.300796Z","shell.execute_reply":"2025-11-06T11:42:25.025595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}